{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train data (use for train & test )\n",
    "path = lambda number:r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\Data\\NormaliezdData\\NormlizedTrainData\"+'\\\\'+str(number)+\".csv\"\n",
    "traindflist=[]\n",
    "DFSIZE=158\n",
    "for i in range(DFSIZE):\n",
    "    df=pd.read_csv( path(i) ).iloc[:,1:]\n",
    "    traindflist.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get test data\n",
    "path = lambda number:r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\Data\\NormaliezdData\\NormilizedTestData\"+'\\\\'+str(number)+\".csv\"\n",
    "testdflist=[]\n",
    "DFSIZE=52\n",
    "for i in range(DFSIZE):\n",
    "    df=pd.read_csv( path(i) ).iloc[:,1:]\n",
    "    testdflist.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputfeature=['total_voltage', 'total_current', 'soc', 'temp_max', 'temp_min',\n",
    "       'motor_voltage', 'motor_current', 'total_P', 'motor_P',\n",
    "       'tempMAXMINdiff', 'SOCgap']\n",
    "outputfeature=['milediff']\n",
    "TimeStep=24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带TimeStep的LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240319, 24, 11), (240319,))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNNX=[]\n",
    "RNNY=[]\n",
    "for df in traindflist:\n",
    "    X=df.loc[:,inputfeature].to_numpy()\n",
    "    y=df.loc[:,outputfeature].to_numpy()\n",
    "    lens=len(df)\n",
    "    for index in range(TimeStep,lens):\n",
    "        RNNX.append(X[index-TimeStep:index])\n",
    "        RNNY.append(y[index-TimeStep:index].sum())\n",
    "RNNX=np.array(RNNX)\n",
    "RNNY=np.array(RNNY)\n",
    "RNNX.shape,RNNY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分为测试集和训练集（可没有此步 ，因为我们有选择测试数据集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #这里是引用了交叉验证\n",
    "X_train,X_test, y_train, y_test = train_test_split(RNNX,RNNY,test_size = 0.1,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216287, 24, 11) (216287,) (24032, 24, 11) (24032,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape , y_train.shape , X_test.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=RNNX\n",
    "y_train=RNNY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , BatchNormalization , Dropout , Activation\n",
    "from keras.layers import LSTM , GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import Adam , SGD , RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr_reduce 设置损失不减则降低学习率\n",
    "### checkPoint设置保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks\\callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "PathName=r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\\RNN\"\n",
    "filepath=PathName+\"\\\\3rd_weights.hdf5\"\n",
    "from keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, epsilon=0.0001, patience=1, verbose=1)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置模型输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE=100\n",
    "BATCH_SIZE=25\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  设置模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape=X_train.shape[-2:]\n",
    "\n",
    "single_step_model = tf.keras.models.Sequential()\n",
    "\n",
    "single_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                           input_shape=input_shape))\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')\n",
    "#single_step_model.compile(loss='mean_squared_error', optimizer=Adam(lr = 0.009) , metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps, validate for 50 steps\n",
      "Epoch 1/30\n",
      "493/500 [============================>.] - ETA: 0s - loss: 0.2516\n",
      "Epoch 00001: val_loss improved from -inf to 0.77696, saving model to C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\\RNN\\3rd_weights.hdf5\n",
      "500/500 [==============================] - 47s 94ms/step - loss: 0.2512 - val_loss: 0.7770\n",
      "Epoch 2/30\n",
      "493/500 [============================>.] - ETA: 0s - loss: 0.1971\n",
      "Epoch 00002: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1959 - val_loss: 0.4087\n",
      "Epoch 3/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1743\n",
      "Epoch 00003: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1743 - val_loss: 0.3353\n",
      "Epoch 4/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.1551\n",
      "Epoch 00004: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1547 - val_loss: 0.3087\n",
      "Epoch 5/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.1327\n",
      "Epoch 00005: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1325 - val_loss: 0.2927\n",
      "Epoch 6/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00006: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.1227 - val_loss: 0.3077\n",
      "Epoch 7/30\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1467\n",
      "Epoch 00007: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.1462 - val_loss: 0.2771\n",
      "Epoch 8/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2497\n",
      "Epoch 00008: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2498 - val_loss: 0.2727\n",
      "Epoch 9/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.3100\n",
      "Epoch 00009: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.3113 - val_loss: 0.2437\n",
      "Epoch 10/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.2811\n",
      "Epoch 00010: val_loss did not improve from 0.77696\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2814 - val_loss: 0.2182\n",
      "Epoch 11/30\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.2615\n",
      "Epoch 00011: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2620 - val_loss: 0.2285\n",
      "Epoch 12/30\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.2448\n",
      "Epoch 00012: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2443 - val_loss: 0.2247\n",
      "Epoch 13/30\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.2849\n",
      "Epoch 00013: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2853 - val_loss: 0.2255\n",
      "Epoch 14/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.3126\n",
      "Epoch 00014: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.3128 - val_loss: 0.2255\n",
      "Epoch 15/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.2401\n",
      "Epoch 00015: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2404 - val_loss: 0.2255\n",
      "Epoch 16/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.2519\n",
      "Epoch 00016: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2510 - val_loss: 0.2255\n",
      "Epoch 17/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.2299\n",
      "Epoch 00017: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2292 - val_loss: 0.2255\n",
      "Epoch 18/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.2478\n",
      "Epoch 00018: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.2476 - val_loss: 0.2255\n",
      "Epoch 19/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2408\n",
      "Epoch 00019: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.2410 - val_loss: 0.2255\n",
      "Epoch 20/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.2119\n",
      "Epoch 00020: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2120 - val_loss: 0.2255\n",
      "Epoch 21/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.2097\n",
      "Epoch 00021: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2106 - val_loss: 0.2255\n",
      "Epoch 22/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.2292\n",
      "Epoch 00022: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.2282 - val_loss: 0.2255\n",
      "Epoch 23/30\n",
      "493/500 [============================>.] - ETA: 0s - loss: 0.1966\n",
      "Epoch 00023: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1953 - val_loss: 0.2255\n",
      "Epoch 24/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1628\n",
      "Epoch 00024: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1630 - val_loss: 0.2255\n",
      "Epoch 25/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.1620\n",
      "Epoch 00025: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1620 - val_loss: 0.2255\n",
      "Epoch 26/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.2427\n",
      "Epoch 00026: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2423 - val_loss: 0.2255\n",
      "Epoch 27/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2374\n",
      "Epoch 00027: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2382 - val_loss: 0.2255\n",
      "Epoch 28/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.3286\n",
      "Epoch 00028: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.3280 - val_loss: 0.2255\n",
      "Epoch 29/30\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.2643\n",
      "Epoch 00029: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2639 - val_loss: 0.2255\n",
      "Epoch 30/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2631\n",
      "Epoch 00030: val_loss did not improve from 0.77696\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2632 - val_loss: 0.2255\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "EVALUATION_INTERVAL=500\n",
    "time_start=time.time()\n",
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=50,callbacks = [checkpoint , lr_reduce])\n",
    "time_end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally cost 198.3333990573883\n"
     ]
    }
   ],
   "source": [
    "print('totally cost',time_end-time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画出模型loss曲线 查看收敛效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ooGDKWUUl7RgKGUUsorGjCUUkp55f8DS+u3qCYQkl8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16703f7f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(single_step_history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (如果有)对上面分出来的测试数据测试（非我们小组定义的测试数据集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.1844077 ],\n",
       "        [-0.06517723],\n",
       "        [ 2.0161567 ],\n",
       "        ...,\n",
       "        [ 1.611495  ],\n",
       "        [ 0.01135138],\n",
       "        [ 2.6155288 ]], dtype=float32), (24032, 1), (24032, 24, 11))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = single_step_model.predict(X_test)\n",
    "pred,pred.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对指定测试集的数据查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集 0 的预测结果是： 125.17151 真实结果是 123.90000000000146\n",
      "相差： 1.2715087890610448\n"
     ]
    }
   ],
   "source": [
    "number=0\n",
    "df=testdflist[number]\n",
    "TestX=[]\n",
    "TestY=[]\n",
    "X=df.loc[:,inputfeature].to_numpy()\n",
    "y=df.loc[:,outputfeature].to_numpy()\n",
    "lens=len(df)\n",
    "for index in range(TimeStep,lens):\n",
    "    if(int(index % TimeStep)==0):\n",
    "        TestX.append(X[index-TimeStep:index])\n",
    "        TestY.append(y[index-TimeStep:index].sum())\n",
    "TestX=np.array(TestX)\n",
    "TestY=np.array(TestY)\n",
    "pred = single_step_model.predict(TestX)\n",
    "pred = pred.cumsum()\n",
    "print(\"测试集\",number,\"的预测结果是：\",pred[-1],\"真实结果是\",df.iloc[-1].milegap)\n",
    "diff=pred[-1]-df.iloc[-1].milegap\n",
    "print(\"相差：\",diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集 0 的预测结果是： 125.17151 真实结果是 123.90000000000146\n",
      "测试集 1 的预测结果是： 100.945015 真实结果是 106.79999999999563\n",
      "测试集 2 的预测结果是： 109.910965 真实结果是 106.90000000000146\n",
      "测试集 3 的预测结果是： 62.08498 真实结果是 54.59999999999855\n",
      "测试集 4 的预测结果是： 113.07635 真实结果是 102.20000000000437\n",
      "测试集 5 的预测结果是： 114.48634 真实结果是 116.5\n",
      "测试集 6 的预测结果是： 102.565605 真实结果是 105.39999999999418\n",
      "测试集 7 的预测结果是： 117.44604 真实结果是 115.69999999999708\n",
      "测试集 8 的预测结果是： 16.85339 真实结果是 18.599999999998552\n",
      "测试集 9 的预测结果是： 98.89158 真实结果是 101.19999999999708\n",
      "测试集 10 的预测结果是： 106.97667 真实结果是 98.0\n",
      "测试集 11 的预测结果是： 125.932816 真实结果是 122.09999999999854\n",
      "测试集 12 的预测结果是： 117.63388 真实结果是 116.29999999999563\n",
      "测试集 13 的预测结果是： 125.422485 真实结果是 115.89999999999418\n",
      "测试集 14 的预测结果是： 139.0267 真实结果是 122.79999999999563\n",
      "测试集 15 的预测结果是： 128.274 真实结果是 114.29999999999563\n",
      "测试集 16 的预测结果是： 150.98763 真实结果是 121.0\n",
      "测试集 17 的预测结果是： 99.82696 真实结果是 126.89999999999418\n",
      "测试集 18 的预测结果是： 116.09512 真实结果是 112.40000000000146\n",
      "测试集 19 的预测结果是： 109.9476 真实结果是 112.60000000000582\n",
      "测试集 20 的预测结果是： 78.42944 真实结果是 77.69999999999709\n",
      "测试集 21 的预测结果是： 115.50091 真实结果是 119.0\n",
      "测试集 22 的预测结果是： 107.69917 真实结果是 117.09999999999854\n",
      "测试集 23 的预测结果是： 8.837518 真实结果是 9.69999999999709\n",
      "测试集 24 的预测结果是： 18.733881 真实结果是 20.29999999999564\n",
      "测试集 25 的预测结果是： 86.62296 真实结果是 98.5\n",
      "测试集 26 的预测结果是： 67.668686 真实结果是 77.29999999999562\n",
      "测试集 27 的预测结果是： 121.77445 真实结果是 120.40000000000146\n",
      "测试集 28 的预测结果是： 96.532906 真实结果是 105.40000000000146\n",
      "测试集 29 的预测结果是： 75.82419 真实结果是 74.59999999999854\n",
      "测试集 30 的预测结果是： 86.206375 真实结果是 83.69999999999709\n",
      "测试集 31 的预测结果是： 108.84173 真实结果是 112.5\n",
      "测试集 32 的预测结果是： 100.80126 真实结果是 113.5\n",
      "测试集 33 的预测结果是： 1.3982588 真实结果是 0.9000000000014552\n",
      "测试集 34 的预测结果是： 93.29136 真实结果是 98.69999999999708\n",
      "测试集 35 的预测结果是： 96.31931 真实结果是 103.39999999999418\n",
      "测试集 36 的预测结果是： 134.59021 真实结果是 129.90000000000146\n",
      "测试集 37 的预测结果是： 100.54451 真实结果是 107.69999999999708\n",
      "测试集 38 的预测结果是： 123.07102 真实结果是 129.89999999999418\n",
      "测试集 39 的预测结果是： 92.857994 真实结果是 99.5\n",
      "测试集 40 的预测结果是： 9.846795 真实结果是 4.0\n",
      "测试集 41 的预测结果是： 13.9473095 真实结果是 16.400000000001455\n",
      "测试集 42 的预测结果是： 51.328995 真实结果是 55.200000000004366\n",
      "测试集 43 的预测结果是： 92.169586 真实结果是 99.0\n",
      "测试集 44 的预测结果是： 47.191006 真实结果是 57.09999999999855\n",
      "测试集 45 的预测结果是： 121.695724 真实结果是 113.69999999999708\n",
      "测试集 46 的预测结果是： 62.889225 真实结果是 68.90000000000146\n",
      "测试集 47 的预测结果是： 121.58223 真实结果是 120.30000000000292\n",
      "测试集 48 的预测结果是： 59.78765 真实结果是 62.80000000000291\n",
      "测试集 49 的预测结果是： 100.46303 真实结果是 96.39999999999418\n",
      "测试集 50 的预测结果是： 109.345146 真实结果是 108.5\n",
      "测试集 51 的预测结果是： 122.59495 真实结果是 117.80000000000292\n"
     ]
    }
   ],
   "source": [
    "difflist=[]\n",
    "for number,df in enumerate(testdflist):\n",
    "    TestX=[]\n",
    "    TestY=[]\n",
    "    X=df.loc[:,inputfeature].to_numpy()\n",
    "    y=df.loc[:,outputfeature].to_numpy()\n",
    "    lens=len(df)\n",
    "    for index in range(TimeStep,lens):\n",
    "        if(int(index % TimeStep)==0):\n",
    "            TestX.append(X[index-TimeStep:index])\n",
    "            TestY.append(y[index-TimeStep:index])\n",
    "    TestX=np.array(TestX)\n",
    "    TestY=np.array(TestY)\n",
    "    pred = single_step_model.predict(TestX)\n",
    "    pred = pred.cumsum()\n",
    "    print(\"测试集\",number,\"的预测结果是：\",pred[-1],\"真实结果是\",df.iloc[-1].milegap)\n",
    "    diff=pred[-1]-df.iloc[-1].milegap\n",
    "    difflist.append(diff/df.iloc[-1].milegap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PathName=r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\\RNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.23285140473790308\n"
     ]
    }
   ],
   "source": [
    "MSE=(sum(np.array(difflist)**2)/52)**(1/2)\n",
    "print(\"MSE:\",MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(filename, inputfeature,outputfeature,TimeStep,MSE,model):\n",
    "    fh = open(filename, 'w', encoding='utf-8')\n",
    "    fh.write(\"inputfeature:\")\n",
    "    for string in inputfeature:\n",
    "        fh.write(string)\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"outputfeature:\")\n",
    "    for string in outputfeature:\n",
    "        fh.write(string)\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"TimeStep:\")\n",
    "    fh.write(str(TimeStep))\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"MSE:\")\n",
    "    fh.write(str(MSE))\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"model:\")\n",
    "    fh.write(model)\n",
    "    fh.write('\\r')\n",
    "    fh.close()\n",
    "save(PathName+\"\\\\3rdDemo.txt\",inputfeature,outputfeature,TimeStep,MSE,\"LSTM(32),Dense(1),optimizer=tf.keras.optimizers.RMSprop(), loss='mae'\")\n",
    "#single_step_model.compile(loss='mean_squared_error', optimizer=Adam(lr = 0.001) , metrics = ['mean_squared_error'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Useless code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#对一个数据集df的数据进行RNN输入化处理\n",
    "X=df.loc[:,inputfeature].to_numpy()\n",
    "y=df.loc[:,outputfeature].to_numpy()\n",
    "newX=X.reshape(X.shape[0],1,X.shape[1])\n",
    "lens=len(newX)\n",
    "RNNX=[]\n",
    "for index,x in enumerate(newX):\n",
    "    if(index<lens-TimeStep):\n",
    "        RNNX.append(X[index:index+TimeStep])\n",
    "    else:\n",
    "        break\n",
    "RNNX=np.array(RNNX)\n",
    "RNNY=[]\n",
    "for index,i in enumerate(y):\n",
    "    if(index<lens-TimeStep):\n",
    "        RNNY.append(y[index:index+TimeStep].sum()*100)\n",
    "    else:\n",
    "        break\n",
    "RNNY=np.array(RNNY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
