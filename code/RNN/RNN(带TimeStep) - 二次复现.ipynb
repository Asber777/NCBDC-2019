{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get train data (use for train & test )\n",
    "path = lambda number:r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\Data\\NormaliezdData\\NormlizedTrainData\"+'\\\\'+str(number)+\".csv\"\n",
    "traindflist=[]\n",
    "DFSIZE=158\n",
    "for i in range(DFSIZE):\n",
    "    df=pd.read_csv( path(i) ).iloc[:,1:]\n",
    "    traindflist.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get test data\n",
    "path = lambda number:r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\Data\\NormaliezdData\\NormilizedTestData\"+'\\\\'+str(number)+\".csv\"\n",
    "testdflist=[]\n",
    "DFSIZE=52\n",
    "for i in range(DFSIZE):\n",
    "    df=pd.read_csv( path(i) ).iloc[:,1:]\n",
    "    testdflist.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputfeature=['total_voltage', 'total_current', 'soc', 'temp_max', 'temp_min',\n",
    "       'motor_voltage', 'motor_current', 'total_P', 'motor_P',\n",
    "       'tempMAXMINdiff', 'SOCgap']\n",
    "outputfeature=['milediff']\n",
    "TimeStep=24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带TimeStep的LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240319, 24, 11), (240319,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNNX=[]\n",
    "RNNY=[]\n",
    "for df in traindflist:\n",
    "    X=df.loc[:,inputfeature].to_numpy()\n",
    "    y=df.loc[:,outputfeature].to_numpy()\n",
    "    lens=len(df)\n",
    "    for index in range(TimeStep,lens):\n",
    "        RNNX.append(X[index-TimeStep:index])\n",
    "        RNNY.append(y[index-TimeStep:index].sum())\n",
    "RNNX=np.array(RNNX)\n",
    "RNNY=np.array(RNNY)\n",
    "RNNX.shape,RNNY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分为测试集和训练集（可没有此步 ，因为我们有选择测试数据集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #这里是引用了交叉验证\n",
    "X_train,X_test, y_train, y_test = train_test_split(RNNX,RNNY,test_size = 0.1,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216287, 24, 11) (216287,) (24032, 24, 11) (24032,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape , y_train.shape , X_test.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=RNNX\n",
    "y_train=RNNY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , BatchNormalization , Dropout , Activation\n",
    "from keras.layers import LSTM , GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import Adam , SGD , RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr_reduce 设置损失不减则降低学习率\n",
    "### checkPoint设置保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks\\callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "PathName=r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\\RNN\"\n",
    "filepath=PathName+\"\\\\3rd_weights.hdf5\"\n",
    "from keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, epsilon=0.0001, patience=1, verbose=1)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置模型输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE=100\n",
    "BATCH_SIZE=25\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  设置模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape=X_train.shape[-2:]\n",
    "\n",
    "single_step_model = tf.keras.models.Sequential()\n",
    "\n",
    "single_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                           input_shape=input_shape))\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')\n",
    "#single_step_model.compile(loss='mean_squared_error', optimizer=Adam(lr = 0.009) , metrics = ['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps, validate for 50 steps\n",
      "Epoch 1/30\n",
      "493/500 [============================>.] - ETA: 0s - loss: 0.2351\n",
      "Epoch 00001: val_loss improved from -inf to 0.86092, saving model to C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\\RNN\\3rd_weights.hdf5\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 0.2347 - val_loss: 0.8609\n",
      "Epoch 2/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.1947\n",
      "Epoch 00002: val_loss did not improve from 0.86092\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1939 - val_loss: 0.4334\n",
      "Epoch 3/30\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.1797\n",
      "Epoch 00003: val_loss did not improve from 0.86092\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1794 - val_loss: 0.3846\n",
      "Epoch 4/30\n",
      "492/500 [============================>.] - ETA: 0s - loss: 0.1617\n",
      "Epoch 00004: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1614 - val_loss: 0.3853\n",
      "Epoch 5/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1830\n",
      "Epoch 00005: val_loss did not improve from 0.86092\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1827 - val_loss: 0.3028\n",
      "Epoch 6/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1490\n",
      "Epoch 00006: val_loss did not improve from 0.86092\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1493 - val_loss: 0.2875\n",
      "Epoch 7/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.1808\n",
      "Epoch 00007: val_loss did not improve from 0.86092\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1802 - val_loss: 0.2660\n",
      "Epoch 8/30\n",
      "493/500 [============================>.] - ETA: 0s - loss: 0.2824\n",
      "Epoch 00008: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.2824 - val_loss: 0.2939\n",
      "Epoch 9/30\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.3517\n",
      "Epoch 00009: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3533 - val_loss: 0.2757\n",
      "Epoch 10/30\n",
      "492/500 [============================>.] - ETA: 0s - loss: 0.3582\n",
      "Epoch 00010: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3588 - val_loss: 0.2746\n",
      "Epoch 11/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.3680\n",
      "Epoch 00011: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3680 - val_loss: 0.2744\n",
      "Epoch 12/30\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.3330\n",
      "Epoch 00012: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3313 - val_loss: 0.2744\n",
      "Epoch 13/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.3535\n",
      "Epoch 00013: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.3577 - val_loss: 0.2744\n",
      "Epoch 14/30\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.3999\n",
      "Epoch 00014: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.4012 - val_loss: 0.2744\n",
      "Epoch 15/30\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.2617\n",
      "Epoch 00015: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2612 - val_loss: 0.2744\n",
      "Epoch 16/30\n",
      "493/500 [============================>.] - ETA: 0s - loss: 0.2159\n",
      "Epoch 00016: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2145 - val_loss: 0.2744\n",
      "Epoch 17/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2343\n",
      "Epoch 00017: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2345 - val_loss: 0.2744\n",
      "Epoch 18/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2934\n",
      "Epoch 00018: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2935 - val_loss: 0.2744\n",
      "Epoch 19/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.2614\n",
      "Epoch 00019: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2609 - val_loss: 0.2744\n",
      "Epoch 20/30\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.2283\n",
      "Epoch 00020: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.2288 - val_loss: 0.2744\n",
      "Epoch 21/30\n",
      "495/500 [============================>.] - ETA: 0s - loss: 0.2387\n",
      "Epoch 00021: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2387 - val_loss: 0.2744\n",
      "Epoch 22/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.2495\n",
      "Epoch 00022: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2487 - val_loss: 0.2744\n",
      "Epoch 23/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.1993\n",
      "Epoch 00023: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1991 - val_loss: 0.2744\n",
      "Epoch 24/30\n",
      "496/500 [============================>.] - ETA: 0s - loss: 0.2375\n",
      "Epoch 00024: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2391 - val_loss: 0.2744\n",
      "Epoch 25/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.2358\n",
      "Epoch 00025: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2356 - val_loss: 0.2744\n",
      "Epoch 26/30\n",
      "498/500 [============================>.] - ETA: 0s - loss: 0.2752\n",
      "Epoch 00026: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2747 - val_loss: 0.2744\n",
      "Epoch 27/30\n",
      "494/500 [============================>.] - ETA: 0s - loss: 0.2558\n",
      "Epoch 00027: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2595 - val_loss: 0.2744\n",
      "Epoch 28/30\n",
      "492/500 [============================>.] - ETA: 0s - loss: 0.3755\n",
      "Epoch 00028: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3751 - val_loss: 0.2744\n",
      "Epoch 29/30\n",
      "497/500 [============================>.] - ETA: 0s - loss: 0.3516\n",
      "Epoch 00029: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000001428009978e-26.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3511 - val_loss: 0.2744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.3690\n",
      "Epoch 00030: val_loss did not improve from 0.86092\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.000000142800998e-27.\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3691 - val_loss: 0.2744\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "EVALUATION_INTERVAL=500\n",
    "time_start=time.time()\n",
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=50,callbacks = [checkpoint , lr_reduce])\n",
    "time_end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally cost 131.5915162563324\n"
     ]
    }
   ],
   "source": [
    "print('totally cost',time_end-time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画出模型loss曲线 查看收敛效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c6b2045f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(single_step_history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (如果有)对上面分出来的测试数据测试（非我们小组定义的测试数据集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.6662506 ],\n",
       "        [-0.09523639],\n",
       "        [ 2.3163042 ],\n",
       "        ...,\n",
       "        [ 1.6203413 ],\n",
       "        [ 0.00898113],\n",
       "        [ 2.597105  ]], dtype=float32), (24032, 1), (24032, 24, 11))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = single_step_model.predict(X_test)\n",
    "pred,pred.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对指定测试集的数据查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集 0 的预测结果是： 143.78136 真实结果是 123.90000000000146\n",
      "相差： 19.881356811521982\n"
     ]
    }
   ],
   "source": [
    "number=0\n",
    "df=testdflist[number]\n",
    "TestX=[]\n",
    "TestY=[]\n",
    "X=df.loc[:,inputfeature].to_numpy()\n",
    "y=df.loc[:,outputfeature].to_numpy()\n",
    "lens=len(df)\n",
    "for index in range(TimeStep,lens):\n",
    "    if(int(index % TimeStep)==0):\n",
    "        TestX.append(X[index-TimeStep:index])\n",
    "        TestY.append(y[index-TimeStep:index].sum())\n",
    "TestX=np.array(TestX)\n",
    "TestY=np.array(TestY)\n",
    "pred = single_step_model.predict(TestX)\n",
    "pred = pred.cumsum()\n",
    "print(\"测试集\",number,\"的预测结果是：\",pred[-1],\"真实结果是\",df.iloc[-1].milegap)\n",
    "diff=pred[-1]-df.iloc[-1].milegap\n",
    "print(\"相差：\",diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集 0 的预测结果是： 143.78136 真实结果是 123.90000000000146\n",
      "测试集 1 的预测结果是： 115.157555 真实结果是 106.79999999999563\n",
      "测试集 2 的预测结果是： 124.001465 真实结果是 106.90000000000146\n",
      "测试集 3 的预测结果是： 67.86873 真实结果是 54.59999999999855\n",
      "测试集 4 的预测结果是： 117.615074 真实结果是 102.20000000000437\n",
      "测试集 5 的预测结果是： 114.45572 真实结果是 116.5\n",
      "测试集 6 的预测结果是： 104.63107 真实结果是 105.39999999999418\n",
      "测试集 7 的预测结果是： 118.05609 真实结果是 115.69999999999708\n",
      "测试集 8 的预测结果是： 20.232481 真实结果是 18.599999999998552\n",
      "测试集 9 的预测结果是： 96.02374 真实结果是 101.19999999999708\n",
      "测试集 10 的预测结果是： 114.26906 真实结果是 98.0\n",
      "测试集 11 的预测结果是： 129.7114 真实结果是 122.09999999999854\n",
      "测试集 12 的预测结果是： 124.837166 真实结果是 116.29999999999563\n",
      "测试集 13 的预测结果是： 131.64833 真实结果是 115.89999999999418\n",
      "测试集 14 的预测结果是： 130.87833 真实结果是 122.79999999999563\n",
      "测试集 15 的预测结果是： 129.34517 真实结果是 114.29999999999563\n",
      "测试集 16 的预测结果是： 141.97038 真实结果是 121.0\n",
      "测试集 17 的预测结果是： 103.69202 真实结果是 126.89999999999418\n",
      "测试集 18 的预测结果是： 119.39316 真实结果是 112.40000000000146\n",
      "测试集 19 的预测结果是： 110.2518 真实结果是 112.60000000000582\n",
      "测试集 20 的预测结果是： 79.127785 真实结果是 77.69999999999709\n",
      "测试集 21 的预测结果是： 119.398155 真实结果是 119.0\n",
      "测试集 22 的预测结果是： 111.1869 真实结果是 117.09999999999854\n",
      "测试集 23 的预测结果是： 8.775828 真实结果是 9.69999999999709\n",
      "测试集 24 的预测结果是： 18.082588 真实结果是 20.29999999999564\n",
      "测试集 25 的预测结果是： 90.31045 真实结果是 98.5\n",
      "测试集 26 的预测结果是： 70.97703 真实结果是 77.29999999999562\n",
      "测试集 27 的预测结果是： 115.23232 真实结果是 120.40000000000146\n",
      "测试集 28 的预测结果是： 88.93467 真实结果是 105.40000000000146\n",
      "测试集 29 的预测结果是： 73.26639 真实结果是 74.59999999999854\n",
      "测试集 30 的预测结果是： 82.48233 真实结果是 83.69999999999709\n",
      "测试集 31 的预测结果是： 106.86199 真实结果是 112.5\n",
      "测试集 32 的预测结果是： 100.948425 真实结果是 113.5\n",
      "测试集 33 的预测结果是： 1.7995732 真实结果是 0.9000000000014552\n",
      "测试集 34 的预测结果是： 93.519394 真实结果是 98.69999999999708\n",
      "测试集 35 的预测结果是： 95.026726 真实结果是 103.39999999999418\n",
      "测试集 36 的预测结果是： 123.879974 真实结果是 129.90000000000146\n",
      "测试集 37 的预测结果是： 94.68227 真实结果是 107.69999999999708\n",
      "测试集 38 的预测结果是： 118.63142 真实结果是 129.89999999999418\n",
      "测试集 39 的预测结果是： 88.238976 真实结果是 99.5\n",
      "测试集 40 的预测结果是： 6.611262 真实结果是 4.0\n",
      "测试集 41 的预测结果是： 16.671251 真实结果是 16.400000000001455\n",
      "测试集 42 的预测结果是： 61.643078 真实结果是 55.200000000004366\n",
      "测试集 43 的预测结果是： 105.887924 真实结果是 99.0\n",
      "测试集 44 的预测结果是： 55.63504 真实结果是 57.09999999999855\n",
      "测试集 45 的预测结果是： 133.60033 真实结果是 113.69999999999708\n",
      "测试集 46 的预测结果是： 75.64253 真实结果是 68.90000000000146\n",
      "测试集 47 的预测结果是： 140.33064 真实结果是 120.30000000000292\n",
      "测试集 48 的预测结果是： 64.341484 真实结果是 62.80000000000291\n",
      "测试集 49 的预测结果是： 110.93832 真实结果是 96.39999999999418\n",
      "测试集 50 的预测结果是： 120.259285 真实结果是 108.5\n",
      "测试集 51 的预测结果是： 131.29765 真实结果是 117.80000000000292\n"
     ]
    }
   ],
   "source": [
    "difflist=[]\n",
    "for number,df in enumerate(testdflist):\n",
    "    TestX=[]\n",
    "    TestY=[]\n",
    "    X=df.loc[:,inputfeature].to_numpy()\n",
    "    y=df.loc[:,outputfeature].to_numpy()\n",
    "    lens=len(df)\n",
    "    for index in range(TimeStep,lens):\n",
    "        if(int(index % TimeStep)==0):\n",
    "            TestX.append(X[index-TimeStep:index])\n",
    "            TestY.append(y[index-TimeStep:index])\n",
    "    TestX=np.array(TestX)\n",
    "    TestY=np.array(TestY)\n",
    "    pred = single_step_model.predict(TestX)\n",
    "    pred = pred.cumsum()\n",
    "    print(\"测试集\",number,\"的预测结果是：\",pred[-1],\"真实结果是\",df.iloc[-1].milegap)\n",
    "    diff=pred[-1]-df.iloc[-1].milegap\n",
    "    difflist.append(diff/df.iloc[-1].milegap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PathName=r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\\RNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.19519778682338101\n"
     ]
    }
   ],
   "source": [
    "MSE=(sum(np.array(difflist)**2)/52)**(1/2)\n",
    "print(\"MSE:\",MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(filename, inputfeature,outputfeature,TimeStep,MSE,model):\n",
    "    fh = open(filename, 'w', encoding='utf-8')\n",
    "    fh.write(\"inputfeature:\")\n",
    "    for string in inputfeature:\n",
    "        fh.write(string)\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"outputfeature:\")\n",
    "    for string in outputfeature:\n",
    "        fh.write(string)\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"TimeStep:\")\n",
    "    fh.write(str(TimeStep))\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"MSE:\")\n",
    "    fh.write(str(MSE))\n",
    "    fh.write('\\r')\n",
    "    fh.write(\"model:\")\n",
    "    fh.write(model)\n",
    "    fh.write('\\r')\n",
    "    fh.close()\n",
    "save(PathName+\"\\\\3rdDemo.txt\",inputfeature,outputfeature,TimeStep,MSE,\"LSTM(32),Dense(1),optimizer=tf.keras.optimizers.RMSprop(), loss='mae'\")\n",
    "#single_step_model.compile(loss='mean_squared_error', optimizer=Adam(lr = 0.001) , metrics = ['mean_squared_error'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Useless code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#对一个数据集df的数据进行RNN输入化处理\n",
    "X=df.loc[:,inputfeature].to_numpy()\n",
    "y=df.loc[:,outputfeature].to_numpy()\n",
    "newX=X.reshape(X.shape[0],1,X.shape[1])\n",
    "lens=len(newX)\n",
    "RNNX=[]\n",
    "for index,x in enumerate(newX):\n",
    "    if(index<lens-TimeStep):\n",
    "        RNNX.append(X[index:index+TimeStep])\n",
    "    else:\n",
    "        break\n",
    "RNNX=np.array(RNNX)\n",
    "RNNY=[]\n",
    "for index,i in enumerate(y):\n",
    "    if(index<lens-TimeStep):\n",
    "        RNNY.append(y[index:index+TimeStep].sum()*100)\n",
    "    else:\n",
    "        break\n",
    "RNNY=np.array(RNNY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
