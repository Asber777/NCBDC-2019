{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def savemodel(model,name):\n",
    "    path=r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\"+'\\\\'+name+'.pkl'\n",
    "    joblib.dump(model,path)\n",
    "def loadmodel(name):\n",
    "    path=r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\"+'\\\\'+name+'.pkl'\n",
    "    return joblib.load('filename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#首先加载数据  #本来需要加载所有数据进行归一化的，但是这里为了简化代码，直接读取归一化输入的训练数据和测试数据\n",
    "trainlist,testlist=[],[]\n",
    "alldata=pd.DataFrame()\n",
    "TestLength=52\n",
    "TrainLength=158\n",
    "for i in range(TrainLength):\n",
    "    df=pd.read_csv(r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\NormaliezdData\\NormlizedTrainData\"+'\\\\'+str(i)+\".csv\").iloc[:,1:]\n",
    "    trainlist.append(df)\n",
    "    alldata=pd.concat([alldata,df])\n",
    "for i in range(TestLength):\n",
    "    df=pd.read_csv(r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\NormaliezdData\\NormilizedTestData\"+'\\\\'+str(i)+\".csv\").iloc[:,1:]\n",
    "    testlist.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single step model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们有这么多的特征： Index(['total_voltage', 'total_current', 'soc', 'temp_max', 'temp_min',\n",
      "       'motor_voltage', 'motor_current', 'total_P', 'motor_P',\n",
      "       'tempMAXMINdiff', 'SOCgap', 'speed', 'mileage', 'milediff', 'speeddiff',\n",
      "       'milegap'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#前面数据都加载好了，现在决定一下我们的模型的输入输出\n",
    "print(\"我们有这么多的特征：\",trainlist[0].columns)\n",
    "#我们将可以输入的特征分类，分成低频特征和高频特征\n",
    "#低频特征： 'soc', 'temp_max', 'temp_min','tempMAXMINdiff','SOCgap'\n",
    "#高频特征：'speed', 'total_voltage', 'total_current', 'motor_voltage', 'motor_current', 'total_P', 'motor_P'\n",
    "#这里的speed有待争议，如果之前的速度有在模型中回归出来，那么可以放到之后时间段作为特征\n",
    "\n",
    "#这里的输出也可以有各种输出，比如timestep范围内的里程增量[milediff].sum()\n",
    "#或者回归最后一个数据距离一开始的里程的里程增量milegap\n",
    "#还可以加上速度speed，或者速度增量speeddiff\n",
    "\n",
    "#这里输出和输入的变化以及模型的调参就交给晓丹，我们这里以一个简单的为例子\n",
    "InputFeatures=['SOCgap','soc', 'total_voltage', 'total_current', 'motor_voltage', 'motor_current', 'total_P', 'motor_P']\n",
    "OutputFeatures=['milediff']\n",
    "TimeStep=120#120*10/60=20mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(120,120):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这个功能主要是把一个df转换成 timestep为一组的df的list 注意 有一些数据集其数据个数<=timestep 会导致输出为[]\n",
    "def df2Timedf(df,inputFeature,outputFeature, timestep):\n",
    "    end_index=len(df)\n",
    "    data=[]\n",
    "    for i in range(timestep,end_index):\n",
    "        indices=range(i-timestep,i)\n",
    "        data.append(df.loc[indices,inputFeature+outputFeature])\n",
    "    if data!=[]:#数据集其数据个数<=timestep 会导致输出为[] 传递None\n",
    "        return data\n",
    "    else:\n",
    "        return \n",
    "#此处是将输入改成（个数，timestep,features）的array,输出（可以改的）是20mins的milediff的和\n",
    "# 可以满足RNN的需要了\n",
    "def Timedf2RNNdata(Timedf,inputFeature,outputFeature):#此函数未处理None Timedf情况\n",
    "    Xset,Yset=[],[]\n",
    "    for df in Timedf:\n",
    "        X=df.loc[:,inputFeature].to_numpy()\n",
    "        Y=df.loc[:,outputFeature].to_numpy()\n",
    "        Xset.append(X)\n",
    "        Yset.append(Y.sum())#注意此处.sum()\n",
    "    Xset=np.array(Xset)\n",
    "    Yset=np.array(Yset)\n",
    "    Yset=np.reshape(Yset,(-1,))\n",
    "    return Xset,Yset\n",
    "#下面是我们的机器学习方法的数据转换，需要将（None，120，8）的数据变成（None，10*8）的数据\n",
    "#也是就说我们每12个数据就需要变成一个数据（取平均）avrgTime=12 (120/12=10)\n",
    "def Timedf2GDBT(Timedf,inputFeature,outputFeatures,avrgTime):\n",
    "    if(Timedf==None):\n",
    "        print(\"Timedf contain nothing\")\n",
    "        return None,None   #如果timedf为None 输出数据也是None，None\n",
    "    Xset,Yset=[],np.array([])\n",
    "    length=len(Timedf[0])\n",
    "    for df in Timedf:\n",
    "        X=df.loc[:,inputFeature]\n",
    "        Y=df.loc[:,outputFeatures]\n",
    "        Xc=np.array([])\n",
    "        for i in range(int(length/avrgTime)):#这里就是 range(10)\n",
    "            begin_index=i*avrgTime\n",
    "            end_index=(i+1)*avrgTime\n",
    "            dataX=np.array(X[begin_index:end_index].mean())\n",
    "            Xc=np.append(Xc,dataX)\n",
    "        Xset.append(Xc)\n",
    "        Yset=np.append(Yset,Y.sum()[0])\n",
    "    Xset=np.array(Xset)\n",
    "    return Xset,Yset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个是读取一个训练集的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=trainlist[3]\n",
    "Timedf=df2Timedf(df,InputFeatures,OutputFeatures,TimeStep)\n",
    "Xset,Yset=Timedf2GDBT(Timedf,InputFeatures,OutputFeatures,avrgTime=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个是读取所有训练集的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dataset is already\n",
      "1 dataset is already\n",
      "2 dataset is already\n",
      "3 dataset is already\n",
      "4 dataset is already\n",
      "5 dataset is already\n",
      "6 dataset is already\n",
      "7 dataset is already\n",
      "8 dataset is already\n",
      "9 dataset is already\n",
      "10 dataset is already\n",
      "11 dataset is already\n",
      "12 dataset is already\n",
      "13 dataset is already\n",
      "14 dataset is already\n",
      "15 dataset is already\n",
      "16 dataset is already\n",
      "17 dataset is already\n",
      "18 dataset is already\n",
      "19 dataset is already\n",
      "20 dataset is already\n",
      "21 dataset is already\n",
      "22 dataset is already\n",
      "23 dataset is already\n",
      "24 dataset is already\n",
      "25 dataset is already\n",
      "26 dataset is already\n",
      "27 dataset is already\n",
      "28 dataset is already\n",
      "29 dataset is already\n",
      "30 dataset is already\n",
      "31 dataset is already\n",
      "32 dataset is already\n",
      "33 dataset is already\n",
      "34 dataset is already\n",
      "35 dataset is already\n",
      "36 dataset is already\n",
      "37 dataset is already\n",
      "38 dataset is already\n",
      "39 dataset is already\n",
      "40 dataset is already\n",
      "41 dataset is already\n",
      "42 dataset is already\n",
      "43 dataset is already\n",
      "44 dataset is already\n",
      "45 dataset is already\n",
      "46 dataset is already\n",
      "47 dataset is already\n",
      "48 dataset is already\n",
      "49 dataset is already\n",
      "50 dataset is already\n",
      "51 dataset is already\n",
      "52 dataset is already\n",
      "53 dataset is already\n",
      "54 dataset is already\n",
      "55 dataset is already\n",
      "56 dataset is already\n",
      "57 dataset is already\n",
      "58 dataset is already\n",
      "60 dataset is already\n",
      "62 dataset is already\n",
      "63 dataset is already\n",
      "64 dataset is already\n",
      "65 dataset is already\n",
      "66 dataset is already\n",
      "67 dataset is already\n",
      "68 dataset is already\n",
      "70 dataset is already\n",
      "71 dataset is already\n",
      "72 dataset is already\n",
      "73 dataset is already\n",
      "74 dataset is already\n",
      "75 dataset is already\n",
      "76 dataset is already\n",
      "77 dataset is already\n",
      "78 dataset is already\n",
      "79 dataset is already\n",
      "80 dataset is already\n",
      "81 dataset is already\n",
      "82 dataset is already\n",
      "83 dataset is already\n",
      "84 dataset is already\n",
      "85 dataset is already\n",
      "86 dataset is already\n",
      "87 dataset is already\n",
      "88 dataset is already\n",
      "89 dataset is already\n",
      "90 dataset is already\n",
      "91 dataset is already\n",
      "92 dataset is already\n",
      "93 dataset is already\n",
      "94 dataset is already\n",
      "95 dataset is already\n",
      "96 dataset is already\n",
      "98 dataset is already\n",
      "99 dataset is already\n",
      "100 dataset is already\n",
      "101 dataset is already\n",
      "102 dataset is already\n",
      "105 dataset is already\n",
      "106 dataset is already\n",
      "107 dataset is already\n",
      "109 dataset is already\n",
      "110 dataset is already\n",
      "111 dataset is already\n",
      "112 dataset is already\n",
      "113 dataset is already\n",
      "114 dataset is already\n",
      "115 dataset is already\n",
      "116 dataset is already\n",
      "117 dataset is already\n",
      "118 dataset is already\n",
      "119 dataset is already\n",
      "120 dataset is already\n",
      "121 dataset is already\n",
      "122 dataset is already\n",
      "123 dataset is already\n",
      "124 dataset is already\n",
      "125 dataset is already\n",
      "126 dataset is already\n",
      "127 dataset is already\n",
      "128 dataset is already\n",
      "129 dataset is already\n",
      "130 dataset is already\n",
      "131 dataset is already\n",
      "132 dataset is already\n",
      "133 dataset is already\n",
      "134 dataset is already\n",
      "135 dataset is already\n",
      "136 dataset is already\n",
      "137 dataset is already\n",
      "138 dataset is already\n",
      "139 dataset is already\n",
      "140 dataset is already\n",
      "141 dataset is already\n",
      "142 dataset is already\n",
      "143 dataset is already\n",
      "144 dataset is already\n",
      "145 dataset is already\n",
      "146 dataset is already\n",
      "147 dataset is already\n",
      "148 dataset is already\n",
      "149 dataset is already\n",
      "150 dataset is already\n",
      "151 dataset is already\n",
      "152 dataset is already\n",
      "153 dataset is already\n",
      "154 dataset is already\n",
      "155 dataset is already\n",
      "156 dataset is already\n",
      "157 dataset is already\n"
     ]
    }
   ],
   "source": [
    "#上面我们定义过 TimeStep=120\n",
    "trainX,trainY=np.empty((1,80)),np.array([])\n",
    "for index,df in enumerate(trainlist[:]):\n",
    "    if(len(df)<=TimeStep):\n",
    "        continue\n",
    "    Timedf=df2Timedf(df,InputFeatures,OutputFeatures,TimeStep)\n",
    "    Xset,Yset=Timedf2GDBT(Timedf,InputFeatures,OutputFeatures,avrgTime=12)\n",
    "    trainX=np.vstack((trainX,Xset))\n",
    "    trainY=np.append(trainY,Yset)\n",
    "    print(index,\"dataset is already\")\n",
    "trainX=trainX[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MyModel(Normalized_X,Normalized_Y,spilt_size=0.1,random_key=0,n_estimators=500,max_depth=4):\n",
    "    ###############################################################################\n",
    "    # Load data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(Normalized_X,Normalized_Y,test_size=spilt_size, random_state=random_key)\n",
    "    ###############################################################################\n",
    "    # Fit regression model\n",
    "    params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    clf = ensemble.GradientBoostingRegressor(**params)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_predict  = clf.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    print(\"MSE: %.4f\" % mse)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (203077, 80) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-e05731878560>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-62-80624c11f3c3>\u001b[0m in \u001b[0;36mMyModel\u001b[1;34m(Normalized_X, Normalized_Y, spilt_size, random_key, n_estimators, max_depth)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m###############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNormalized_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNormalized_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspilt_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m###############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Fit regression model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2124\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[1;32m-> 2124\u001b[1;33m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[0;32m   2125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    217\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    218\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (203077, 80) and data type float64"
     ]
    }
   ],
   "source": [
    "model=MyModel(trainX,trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试方法：先将测试集数据转换成输入，然后得到测试集输出，和测试集真实里程差作比较（最后的里程差）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面是之前写的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0030\n"
     ]
    }
   ],
   "source": [
    "TIMESTEP=3    \n",
    "FRAGEMENT=0  \n",
    "NAME=\"1stNoC\"    \n",
    "Scaler = StandardScaler().fit(alldata.iloc[:,4:-3].to_numpy())    \n",
    "conti_X,conti_Y=[],[]    \n",
    "getTimeSdata2ConBox(x_Nocharing,y_Nocharing,scaler=Scaler,number=FRAGEMENT,conti_X=conti_X,conti_Y=conti_Y,TIMESTEP=TIMESTEP)    \n",
    "XX,YY=timeS2sigle(conti_X,conti_Y,TIMESTEP=TIMESTEP)    \n",
    "conti_X,conti_Y=[],[]    \n",
    "model=MyModel(XX,YY)    \n",
    "e,r=error(model,x_Nocharing,y_Nocharing,\"1stNoC\")    \n",
    "savemodel(model,NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0045\n"
     ]
    }
   ],
   "source": [
    "TIMESTEP=3\n",
    "FRAGEMENT=0\n",
    "NAME=\"ALLNOC\"\n",
    "Scaler = StandardScaler().fit(alldata.iloc[:,4:-3].to_numpy())\n",
    "conti_X,conti_Y=[],[]\n",
    "for i in range(132):\n",
    "    getTimeSdata2ConBox(x_Nocharing,y_Nocharing,scaler=Scaler,number=i,conti_X=conti_X,conti_Y=conti_Y,TIMESTEP=TIMESTEP)\n",
    "XX,YY=timeS2sigle(conti_X,conti_Y,TIMESTEP=TIMESTEP)\n",
    "conti_X,conti_Y=[],[]\n",
    "model=MyModel(XX,YY)\n",
    "e,r=error(model,x_Nocharing,y_Nocharing,NAME)\n",
    "savemodel(model,NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  562,  584],\n",
       "       [   1,  694,  733],\n",
       "       [   2,  143,  146],\n",
       "       [   3,   89,   92],\n",
       "       [   4,  100,  109],\n",
       "       [   5,    3,    3],\n",
       "       [   6,    2,    3],\n",
       "       [   7,  400,  421],\n",
       "       [   8,    8,    8],\n",
       "       [   9,    3,    4],\n",
       "       [  10,   31,   32],\n",
       "       [  11,  337,  354],\n",
       "       [  12,   19,   20],\n",
       "       [  13,    7,    7],\n",
       "       [  14,    5,    5],\n",
       "       [  15,    4,    5],\n",
       "       [  16,    7,    9],\n",
       "       [  17,    3,    4],\n",
       "       [  18,  669,  726],\n",
       "       [  19,   17,   18],\n",
       "       [  20,   16,   18],\n",
       "       [  21,  494,  496],\n",
       "       [  22,  438,  458],\n",
       "       [  23,  233,  234],\n",
       "       [  24,    6,    4],\n",
       "       [  25,    5,    4],\n",
       "       [  26,  821,  814],\n",
       "       [  27,  128,  132],\n",
       "       [  28,  397,  404],\n",
       "       [  29,   13,   12],\n",
       "       [  30,  356,  362],\n",
       "       [  31,  104,  102],\n",
       "       [  32,   11,   11],\n",
       "       [  33,  203,  201],\n",
       "       [  34,    4,    4],\n",
       "       [  35,  110,  112],\n",
       "       [  36,   91,   88],\n",
       "       [  37,    7,    7],\n",
       "       [  38,    7,    7],\n",
       "       [  39,   70,   73],\n",
       "       [  40,  516,  521],\n",
       "       [  41,   12,   13],\n",
       "       [  42,   64,   59],\n",
       "       [  43,    7,    7],\n",
       "       [  44,    6,    6],\n",
       "       [  45,    8,   10],\n",
       "       [  46,    8,    8],\n",
       "       [  47,   19,   20],\n",
       "       [  48,  771,  784],\n",
       "       [  49, 2247, 2266],\n",
       "       [  50, 2899, 2998],\n",
       "       [  51,  862,  845],\n",
       "       [  52,  455,  432],\n",
       "       [  53,    4,    3],\n",
       "       [  54,   44,   42],\n",
       "       [  55,   16,   14],\n",
       "       [  56,    2,    2],\n",
       "       [  57,    5,    6],\n",
       "       [  58,  257,  250],\n",
       "       [  59,    4,    4],\n",
       "       [  60,    5,    3],\n",
       "       [  61,  110,  110],\n",
       "       [  62,  214,  205],\n",
       "       [  63,  493,  475],\n",
       "       [  64,    6,    5],\n",
       "       [  65,    9,   11],\n",
       "       [  66,  385,  374],\n",
       "       [  67,   13,   12],\n",
       "       [  68,  468,  443],\n",
       "       [  69,    6,    6],\n",
       "       [  70,   11,   10],\n",
       "       [  71,  228,  218],\n",
       "       [  72,   52,   49],\n",
       "       [  73,  239,  224],\n",
       "       [  74,   18,   17],\n",
       "       [  75,  121,  112],\n",
       "       [  76,   29,   27],\n",
       "       [  77,   10,   10],\n",
       "       [  78,   10,   10],\n",
       "       [  79,  708,  679],\n",
       "       [  80, 1358, 1273],\n",
       "       [  81,  369,  346],\n",
       "       [  82,  188,  178],\n",
       "       [  83, 2590, 2420],\n",
       "       [  84,  827,  794],\n",
       "       [  85,   10,    9],\n",
       "       [  86,    4,    4],\n",
       "       [  87,  586,  556],\n",
       "       [  88,  417,  402],\n",
       "       [  89,   34,   33],\n",
       "       [  90,  175,  162],\n",
       "       [  91,    0,    0],\n",
       "       [  92,   63,   58],\n",
       "       [  93,    3,    4],\n",
       "       [  94,   11,   10],\n",
       "       [  95,    9,    7],\n",
       "       [  96,    5,    5],\n",
       "       [  97,    5,    6],\n",
       "       [  98,    1,    2],\n",
       "       [  99,    8,    8],\n",
       "       [ 100,  198,  189],\n",
       "       [ 101,  593,  560],\n",
       "       [ 102,  218,  198],\n",
       "       [ 103, 1180, 1075],\n",
       "       [ 104, 2550, 2456],\n",
       "       [ 105,  258,  262],\n",
       "       [ 106,    8,    8],\n",
       "       [ 107,   49,   54],\n",
       "       [ 108,    6,    6],\n",
       "       [ 109,  577,  606],\n",
       "       [ 110,    3,    3],\n",
       "       [ 111,    4,    4],\n",
       "       [ 112,  691,  715],\n",
       "       [ 113,   12,   12],\n",
       "       [ 114,    3,    3],\n",
       "       [ 115,  107,  112],\n",
       "       [ 116,   54,   53],\n",
       "       [ 117,    3,    3],\n",
       "       [ 118,  145,  151],\n",
       "       [ 119,   45,   51],\n",
       "       [ 120,  205,  216],\n",
       "       [ 121,  230,  243],\n",
       "       [ 122,  200,  205],\n",
       "       [ 123,  582,  614],\n",
       "       [ 124,  487,  509],\n",
       "       [ 125,  407,  419],\n",
       "       [ 126, 1129, 1170],\n",
       "       [ 127,  337,  350],\n",
       "       [ 128,  504,  518],\n",
       "       [ 129,  420,  473],\n",
       "       [ 130,  214,  235],\n",
       "       [ 131, 1796, 1968]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.loadtxt(r\"C:\\Users\\14020\\Desktop\\NCBDC 2019\\model\"+'\\\\'+\"ALLNoC286.123800192\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
